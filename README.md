# Project B: State Space Models and Vision-Language Models

## Overview
This repository contains code for Project B, which explores State Space Models (SSMs) and Vision-Language Models (VLMs). The project is divided into two parts:

- **Part 1 & 2**: State Space Models (SSMs) — Implemented in `CompressViT.ipynb`.
- **Part 3 & 4**: Vision-Language Models (VLMs) — Implemented in `CapMamba.ipynb`.

The first notebook, `CompressViT.ipynb`, explores extensions to the Mamba model for the image captioning task. The second, `CapMamba.ipynb`, addresses the efficiency bottleneck of the Qwen2-VL model by applying an alternative method, token pruning, to compress the data.

Thanks to `vit_python` for providing the root class of the models used in both notebooks.
